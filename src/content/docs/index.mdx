---
title: swiftide
description: Blazing fast, streaming indexing and query library for Retrieval Augment Generation (RAG), written in Rust
template: splash
hero:
  tagline: Blazing fast, streaming indexing and query library for Retrieval Augment Generation (RAG), written in Rust
  image:
    file: ../../assets/logo-full.png
  actions:
    - text: What is swiftide?
      link: /what-is-swiftide/
      icon: right-arrow
      variant: primary
    - text: View it on Github
      link: https://github.com/bosun-ai/swiftide
      icon: external
      variant: secondary
---

import { Card, CardGrid } from "@astrojs/starlight/components";

<div class="flex flex-row not-content justify-around mb-20">
  <div class="basis-1/3">
    <h2>What is Swiftide?</h2>
    <p>
      Swiftide is a Rust native library for building LLM applications. Large language models are amazing, but need context
      to solve real problems. Swiftide allows you to ingest, transform and index large amounts of data fast, and then query that data so it it can be injected into prompts.
      This process is called Retrieval Augmented Generation.
      </p>

  </div>
  <div class="basis-1/3">
    <h2>Why Swiftide?</h2>
    <p>Production LLM applications deal with large amounts of data, concurrent LLM requests and structured and unstructured transformations of data. Rust is great at this. The goal
      of Swiftide is to build indexing and query pipelines easily, experiment and verify, then ship it right to production.</p>
  </div>
</div>

<details open>
  <summary>A quick example</summary>

    ```rust
    indexing::Pipeline::from_loader(FileLoader::new(".").with_extensions(&["md"]))
            .with_default_llm_client(openai_client)
            .then_chunk(ChunkMarkdown::from_chunk_range(10..512))
            .then(MetadataQAText::default())
            .then(move |mut node: Node| {
              node.metadata.insert("Hello", "Metadata");

              Ok(node)
            })
            .then_in_batch(256, Embed::new(FastEmbed::default()))
            .then_store_with(
                Qdrant::builder()
                    .batch_size(50)
                    .vector_size(384)
                    .collection_name("swiftide-examples")
                    .build()?,
            )
            .run()
            .await?;


    query::Pipeline::default()
        .then_transform_query(GenerateSubquestions::from_client(
            openai_client.clone(),
        ))
        .then_transform_query(Embed::from_client(
            openai_client.clone(),
        ))
        .then_retrieve(qdrant.clone())
        .then_answer(Simple::from_client(openai_client.clone()))
        .query("How can I use the query pipeline in Swiftide?")
        .await?;
    ```

</details>
<br />
<CardGrid stagger>
	<Card title="Transform, enrich and persist lots of data" icon="seti:bicep">
  Load data from various sources, transform it, enrich it with metadata, and persist it with lazy, asynchronous, parallel pipelines.
</Card>
	<Card title="Experimental query pipeline" icon="seti:bicep">
    Augment queries with retrieved data using the streaming query pipeline and generate a response.
</Card>
	<Card title="Customizable templated prompts" icon="document">
    Customize and bring your own prompts, build on `Tera`, a jinja style templating library.
</Card>

<Card title="Many existing integrations" icon="puzzle">
  Qdrant, OpenAI, Groq, AWS Bedrock, Redis, FastEmbed, Spider and many more.
</Card>
	<Card title="Easy to extend" icon="seti:crystal_embedded">
  Write your own loaders, transformers, and storages by extending straight forward traits.
</Card>
	<Card title="Written in Rust" icon="seti:rust">
  Fast, safe, and efficient. Built with Rust's async and streaming features.
</Card>
	<Card title="Part of Bosun.ai" icon="rocket">
  Part of [Bosun.ai](https://bosun.ai) and actively used in production.
</Card>
<Card title="Reference" icon="open-book">
  Full API documentation available on [docs.rs](https://docs.rs/swiftide/latest/swiftide/)
</Card>
</CardGrid>
